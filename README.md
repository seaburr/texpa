# texpa

*Transformer experiment A.*

I have no idea how LLMs actually work. This is an experiment in using an LLM (GPT-4-turbo) to build one from scratch in 
Python. This is Experiment A and will probably be abandoned quickly.

## Goal

End goal is to build a (bad) LLM or *something* from scratch while writing virtually none of the code myself. I'm going prompt engineer
this code and any error produced will be given to to the LLM  to troubleshoot. If I write any code, it'll simply be moving
things around and massaging the tools.

## Training Data

I've decided to use a random assortment of books from Project Gutenberg as my base though mostly famous novels.

## Usage

Edit `query.py` with the start of your prompt then run `query.py`.
